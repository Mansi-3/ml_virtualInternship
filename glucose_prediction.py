# -*- coding: utf-8 -*-
"""glucose_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zuZsih8zLZOLV8hygVgoda1IFUrwDoqD

Step 1: Import Libraries and Load Data
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
df = pd.read_csv('framingham.csv')

# Inspect the data
print("First 5 rows:")
print(df.head())
print("\nData Info:")
print(df.info())

"""Step 2: Data Cleaning"""

# Check for missing values
print("\nMissing Values before cleaning:")
print(df.isnull().sum())

# Drop rows with missing values (simple approach)
df_clean = df.dropna()
print(f"\nShape after cleaning: {df_clean.shape}")

"""Step 3: Exploratory Data Analysis (EDA)"""

# 1. Distribution of Glucose
plt.figure(figsize=(10, 6))
sns.histplot(df_clean['glucose'], kde=True, bins=30, color='blue')
plt.title('Distribution of Glucose Levels')
plt.xlabel('Glucose (mg/dL)')
plt.ylabel('Count')
plt.show()

# 2. Correlation Heatmap
plt.figure(figsize=(12, 10))
# Calculate correlation matrix
corr = df_clean.corr()
sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Correlation Matrix of Health Features')
plt.show()

"""Step 4: Feature Selection & Engineering"""

# Create binary target: High Glucose Risk (e.g., Glucose > 100 mg/dL)
# 100 mg/dL is a common threshold for pre-diabetes/diabetes screening.
df_clean['High_Glucose'] = (df_clean['glucose'] > 100).astype(int)

# Define Features (X) and Target (y)
# We exclude 'glucose' (source of target), 'High_Glucose' (target itself),
# 'diabetes' (direct correlation), and 'education' (often less relevant).
feature_cols = ['male', 'age', 'currentSmoker', 'cigsPerDay', 'BPMeds',
                'prevalentStroke', 'prevalentHyp', 'totChol', 'sysBP', 'diaBP',
                'BMI', 'heartRate']

X = df_clean[feature_cols]
y = df_clean['High_Glucose']

# Split into Training and Testing sets (80% Train, 20% Test)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"Training shape: {X_train.shape}")
print(f"Testing shape: {X_test.shape}")

"""Step 5 & 6: Train Models and Evaluate"""

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix

# Initialize models
# Note: We use class_weight='balanced' to handle the fact that fewer people have high glucose
models = {
    "Logistic Regression": LogisticRegression(max_iter=2000, solver='liblinear'),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
}

# Train and Evaluate
for name, model in models.items():
    print(f"\n--- {name} ---")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # Metrics
    acc = accuracy_score(y_test, y_pred)
    print(f"Accuracy: {acc:.4f}")
    print("Classification Report:")
    print(classification_report(y_test, y_pred))

"""Step 7 & 8: Feature Importance & Insights"""

# Extract feature importance from Random Forest
rf_model = models['Random Forest']
importances = rf_model.feature_importances_
indices = np.argsort(importances)[::-1]

# Plot Feature Importance
plt.figure(figsize=(10, 6))
plt.title("Feature Importance (Random Forest)")
plt.bar(range(X.shape[1]), importances[indices], align="center", color='green')
plt.xticks(range(X.shape[1]), [feature_cols[i] for i in indices], rotation=45, ha='right')
plt.tight_layout()
plt.show()

# Print Top Features
print("\nTop 5 Important Features:")
for i in range(5):
    print(f"{i+1}. {feature_cols[indices[i]]}")

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# 1. Load and Prepare Data
df = pd.read_csv('framingham.csv').dropna()
# Create Target: 1 if Glucose > 100, else 0
df['High_Glucose'] = (df['glucose'] > 100).astype(int)

features = ['male', 'age', 'currentSmoker', 'cigsPerDay', 'BPMeds',
            'prevalentStroke', 'prevalentHyp', 'totChol', 'sysBP', 'diaBP',
            'BMI', 'heartRate']
X = df[features]
y = df['High_Glucose']

# 2. Train an Improved Random Forest Model
# - n_estimators=200: More trees for stability
# - max_depth=10: Allows deeper trees to capture complex patterns
# - class_weight='balanced_subsample': heavily penalizes missing the minority class
model = RandomForestClassifier(n_estimators=200, max_depth=10,
                               random_state=42, class_weight='balanced_subsample')
model.fit(X, y)

# 3. Define Test Cases
test_cases = [
    # Case 1: Healthy Young Male (Expected: Low Risk)
    {'male': 1, 'age': 30, 'currentSmoker': 0, 'cigsPerDay': 0, 'BPMeds': 0, 'prevalentStroke': 0, 'prevalentHyp': 0, 'totChol': 180, 'sysBP': 110, 'diaBP': 70, 'BMI': 22.5, 'heartRate': 70, 'Label': 'Healthy Young Male'},
    # Case 2: At-Risk Senior Female (Expected: Moderate/High Risk)
    {'male': 0, 'age': 65, 'currentSmoker': 0, 'cigsPerDay': 0, 'BPMeds': 1, 'prevalentStroke': 0, 'prevalentHyp': 1, 'totChol': 260, 'sysBP': 160, 'diaBP': 100, 'BMI': 35.0, 'heartRate': 80, 'Label': 'At-Risk Senior Female'},
    # Case 3: Real Positive Case (Expected: High Risk)
    {'male': 1, 'age': 53, 'currentSmoker': 1, 'cigsPerDay': 10, 'BPMeds': 0, 'prevalentStroke': 0, 'prevalentHyp': 1, 'totChol': 229, 'sysBP': 147, 'diaBP': 82, 'BMI': 27.8, 'heartRate': 60, 'Label': 'Known High Risk Profile'}
]
test_df = pd.DataFrame(test_cases)

# 4. Predict with a Custom Threshold
# Standard is 0.5, but for health screening, we often lower it to catch more cases.
THRESHOLD = 0.35

probs = model.predict_proba(test_df[features])[:, 1]
custom_preds = (probs >= THRESHOLD).astype(int)

# 5. Show Results
results = pd.DataFrame({
    'Test Scenario': test_df['Label'],
    'Risk Probability': probs.round(3),
    'Prediction (Threshold 0.35)': ['High Risk' if p == 1 else 'Low Risk' for p in custom_preds]
})

print(results)